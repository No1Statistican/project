---
title: "API Example - Twitter Data"
author: Adrianne Bradford
date: February 19, 2018
output:
  html_document
---

# rtweet
** Demonstration is adapted from rtweet documentation - https://github.com/mkearney/rtweet **

##Loading rtweet
First, run this code to install and load the library, rtweet.  We're using the package "devtools" to install the most recent version of rtweet directly from the author's github repository.  Sometimes the CRAN repository doesn't contain the most recent version of the package, especially if the newer code in github is considered a "beta" release.  We're going to use the news version from github because the authentication with Twitter has been made substantially easier.

NOTE: You will also need a Twitter account to run this code.
```{r warning=FALSE}
if (!requireNamespace("devtools", quietly = TRUE)) {
  install.packages("devtools")
}

## install dev version of rtweet from github
devtools::install_github("mkearney/rtweet")

library(rtweet)
library(httpuv)
```

## search_tweets
Firstly, we can use the Twitter search API to pull previously tweeted tweets - this function behaves the same as doing a search in the twitter search bar, returns a bunch of variables in a JSON format and then rtweet turns it into a dataframe for us.

In this example, we're searching for tweets that include the hashtag #rstats, and limiting our results to 18,000 maximum - you will likely get fewer results due to rate limiting. The include_rts argument indicates whether or not you want your results to include retweets.  In this example we have set that to FALSE, so we would only get original tweets.  Search_tweets using the standard Twitter API (the free version) will only pull historical tweets within the past week.

NOTE:  On your first API request, a browser window will pop-up - log into your twitter account to authenticate.
```{r cache=TRUE}
rt <- search_tweets(
  "#rstats", n = 18000, include_rts = FALSE
)
```

## Variables in Twitter Data
```{r}
# LIST VARIABLE NAMES
colnames(rt)
```
```{r echo=FALSE}
if (!requireNamespace("ggplot2", quietly = TRUE)) {
  install.packages("ggplot2")
}
library(ggplot2)
```
## Using Collected Tweet Data - Tweets over Time
We can make various plots using this saved data, for example, graphing the number of tweets by timeperiod.
```{r warning=FALSE}
ts_plot(rt, "3 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of #rstats Twitter statuses from past week",
    subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )
```

```{r echo=FALSE}
if (!requireNamespace("plyr", quietly = TRUE)) {
  install.packages("plyr")
}
library(plyr)
```

## Using Collected Twitter Data - Frequency Table
Or we can look at a frequency table of the "source" of the tweets - what application a user is using in order to send their tweets.
```{r}
count(rt, 'source')
```

```{r echo=FALSE}
if (!requireNamespace("maps", quietly = TRUE)) {
  install.packages("maps")
}
library(maps)
```
## Using Collected Twitter Data - Mapping
Or we can look at the prevalence of #rstats tweets in the USA by location
```{r warning=FALSE, cache=TRUE}
## search for 10,000 tweets sent from the US
us <- search_tweets(
  "#rstats", geocode = lookup_coords("usa"), n = 10000
)
```

##Mapping #2
```{r warning=FALSE}
## create lat/lng variables using all available tweet and profile geo-location data
us <- lat_lng(us)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)

## plot lat and lng points onto state map
with(us, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```

# "Listening" to Twitter

## stream_tweets
So far we have done our analysis on past tweets through the search API.  For some applications, you will want to "listen" to the twitter stream and obtain a 1% sample of all tweets as they are posted.  

The most basic is obtaining a random sample, using no filters.  rtweet will default to listenting for 30 seconds unless otherwise specified.
```{r cache=TRUE, results='hide'}
st <- stream_tweets("")
```

## What do the tweets say?
```{r}
#PRINT TEXT OF FIRST 10 TWEETS IN TABLE
head(st["text"], n=10)
```
Some of what you receive in the text might look like gibberish.  Values like <U+3044> indicate emoji or special characters.  This data would need cleaning before performing text analysis.

##Filtering the Stream
You can also listen for specific filter items (search terms).  Here we'll listen for tweets about Donald Trump.  The 60 in the timeout indicates we want to listen for 60 seconds (a full minute).
```{r cache=TRUE, results='hide'}
djt = stream_tweets(
  "realdonaldtrump,trump", 
  timeout = 60, include_rts = FALSE
)
```

## What do our Trump tweets say?
```{r}
#PRINT TEXT OF FIRST 10 TWEETS IN TABLE
head(djt["text"], n=10)
```

## Mapping Trump Tweets
We can again map the tweets using the geotag.  
```{r}
## create lat/lng variables using all available tweet and profile geo-location data
djt <- lat_lng(djt)
## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)
## plot lat and lng points onto state map
with(djt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```
Geotagging of tweets is optional plus we didn't limit to the US only, so we may see fewer points than we would expect.

##Listening Over Time
IMPORTANT: If you're going to leave your computer on to run this over time, make sure to set your power and sleep settings so that the PC will not go to sleep.  If it does, your code will stop and you will lose data and have to restart it,
```{r eval=FALSE}
## Stream keywords used to filter tweets
q <- "hillaryclinton,imwithher,realdonaldtrump,maga,electionday"  ## Put your selected filter terms here, 
##separated by commas.  The commas serve as an OR search.

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes

twoweeks <- 60L * 60L * 24L * 7L * 2L  

##USE the function stream_tweets2 so that the stream automatically reconnects after an interruption 
##(such as temporary loss of internet).  If your computer restarts you will have to manually restart 
##your code to keep it going.  Use a different name in the dir each time, then you can parse each of 
##your json files and append the data frames together for analysis.

stream_tweets2(
  q,
  parse = FALSE,
  timeout = twoweeks,
  dir = "election-stream" ## This code saves the raw JSON twitter data in a file election-stream.json
)
```

##Parsing JSONS and appending dataframes
```{r eval=FALSE}
## Parse from json file to get a usable dataframe
rt <- parse_stream("election-stream.json")
rt2 <- parse_stream("election-stream2.json")
all_tweets <- rbind(rt, rt2)


```

## Monitor Your Output
You may want to use a series of shorter listening times (such as 1 day) and at the end of each day inspect the tweets you've received for trends so that you can add any emerging hashtags to your filter terms.

## Very Basic Text Mining - Bag of Words
To see what popular terms are in your collected tweets, we'll use a text mining package, qdap, to look at frequent terms.  For this example I'm using the Donald Trump tweets from before.  Qdap will require you to have Java installed on your computer matching (32-bit vs. 64-bit) your version of R. If you get an error when loading qdap, Get the right version of Java at https://www.java.com/en/download/manual.jsp.
```{r warning=FALSE, results='hide'}
if (!requireNamespace("qdap", quietly = TRUE)) {
  install.packages("qdap")
}
library(qdap)
```
## The Bag of Trump Words
```{r}
frequent_terms <- freq_terms(djt["text"], 30)
plot(frequent_terms)
```

##Subsetting to tweets by filter
After you have collected your tweets, you may find there are some tweets that don't match what you're specifically looking for.  If I wanted Donald Trump tweets without comments about Russia or Russians, I would do the following:
```{r include=FALSE}
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}
if (!requireNamespace("stringr", quietly = TRUE)) {
  install.packages("stringr")
}
library(dplyr)
library(stringr)
```
```{r}
djt_without <- djt%>%filter(str_detect(str_to_lower(text), "russia") == FALSE)
```
The data frame goes from 2566 tweets to 1890 tweets.

More about filtering and subsetting text data at https://blog.exploratory.io/filter-with-text-data-952df792c2ba.